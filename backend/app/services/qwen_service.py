"""
Qwen model service for document classification
–°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ:
- –ü—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: –ø–æ–ª—É—á–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç RAG, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ Redis
- –ü—Ä–∏ –ø–æ–∏—Å–∫–µ: –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ RAG/Postgres, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç, –ø–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Redis
"""
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Optional, Dict, Any
import logging
import os
import json
import time
from app.core.config import settings

logger = logging.getLogger(__name__)


class QwenService:
    """Service for Qwen model operations"""
    
    _instance = None
    _model = None
    _tokenizer = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(QwenService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        # Lazy loading - model will be loaded on first use
        # This prevents blocking startup if model download is needed
        pass
    
    def _ensure_model_loaded(self):
        """Ensure model is loaded (lazy loading) - –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏"""
        
        if self._model is None or self._tokenizer is None:
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Qwen –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏ (lazy loading, –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å)...")
            try:
                self._load_model()
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
            except Exception as e:
                logger.error(f"‚ùå Failed to load Qwen model: {e}", exc_info=True)
                raise
    
    def get_memory_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏ GPU"""
        info = {
            "model_loaded": self._model is not None,
            "tokenizer_loaded": self._tokenizer is not None,
        }
        
        if torch.cuda.is_available():
            info["cuda_available"] = True
            info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated() / 1024**3:.2f} GB"
            info["gpu_memory_reserved"] = f"{torch.cuda.memory_reserved() / 1024**3:.2f} GB"
            info["gpu_memory_total"] = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB"
        else:
            info["cuda_available"] = False
        
        return info
    
    def _load_model(self):
        """Load Qwen model - –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ (lazy loading)"""
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ Qwen (lazy loading)...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Qwen3-4B –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –ø—Ä–æ–µ–∫—Ç–µ)
        model_name = settings.QWEN_MODEL_NAME
        use_local = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (—Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤, –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏)
        if settings.QWEN_MODEL_PATH:
            model_path = settings.QWEN_MODEL_PATH
            index_file = os.path.join(model_path, "model.safetensors.index.json")
            
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ —á—Ç–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
            if os.path.isdir(model_path) and os.path.isfile(index_file):
                try:
                    # –¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤, –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –∏—Ö
                    with open(index_file, 'r') as f:
                        index_data = json.load(f)
                    
                    weight_map = index_data.get('weight_map', {})
                    required_files = set(weight_map.values())
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤, –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ–º
                    all_files_exist = all(
                        os.path.isfile(os.path.join(model_path, fname))
                        for fname in required_files
                    )
                    
                    if all_files_exist:
                        use_local = True
                        model_name = model_path
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {model_path}, –Ω–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É...")
                    else:
                        missing = [f for f in required_files if not os.path.isfile(os.path.join(model_path, f))]
                        logger.warning(f"‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–ø–æ–ª–Ω–∞—è, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã: {missing[:5]}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            else:
                logger.info(f"üì• –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {model_path}, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ Hugging Face")
        
        if not use_local:
            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face: {model_name}")
            logger.warning("‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ...")
        
        device = self._get_best_device()
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {model_name} –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {device}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è GPU vs CPU
        if device == "cuda":
            # –í—ã—á–∏—Å–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏ (–≤ –±–∞–π—Ç–∞—Ö)
            max_memory_gb = settings.QWEN_MAX_MEMORY_PERCENT / 100.0
            total_memory_bytes = torch.cuda.get_device_properties(0).total_memory
            max_memory_bytes = int(total_memory_bytes * max_memory_gb)
            max_memory = {0: f"{max_memory_bytes // (1024**3)}GiB"}  # –§–æ—Ä–º–∞—Ç –¥–ª—è accelerate
            
            model_kwargs = {
                "dtype": torch.float16,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è GPU (–±—ã—Å—Ç—Ä–µ–µ –∏ –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏)
                "device_map": "auto",  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ GPU
                "max_memory": max_memory,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
                "trust_remote_code": True,
                "local_files_only": use_local,
                "torch_dtype": torch.float16,  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º dtype –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            }
            logger.info(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU: {settings.QWEN_MAX_MEMORY_PERCENT}% –¥–ª—è –º–æ–¥–µ–ª–∏, {100 - settings.QWEN_MAX_MEMORY_PERCENT}% –¥–ª—è –±—É—Ñ–µ—Ä–∞")
        else:
            model_kwargs = {
                "dtype": torch.float32,  # Always use float32 for CPU compatibility
                "device_map": None,  # Explicitly set to None for CPU
                "trust_remote_code": True,
                "local_files_only": use_local
            }
        
        # Quantization –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ (–æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è Mac)
        if settings.QWEN_LOAD_IN_8BIT:
            model_kwargs["load_in_8bit"] = True
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 8-bit quantization")
        elif settings.QWEN_LOAD_IN_4BIT:
            model_kwargs["load_in_4bit"] = True
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 4-bit quantization (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Mac)")
        
        try:
            logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
            # –î–ª—è Qwen3 –∏—Å–ø–æ–ª—å–∑—É–µ–º Qwen2Tokenizer (Qwen3 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ—Ç –∂–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä)
            try:
                from transformers import Qwen2Tokenizer
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º Qwen2Tokenizer –¥–ª—è Qwen3 –º–æ–¥–µ–ª–∏...")
                self._tokenizer = Qwen2Tokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True,
                    local_files_only=use_local
                )
            except (ImportError, Exception) as tokenizer_error:
                logger.warning(f"‚ö†Ô∏è Qwen2Tokenizer –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({tokenizer_error}), –ø—Ä–æ–±—É–µ–º AutoTokenizer...")
                # Fallback –Ω–∞ AutoTokenizer
                self._tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True,
                    local_files_only=use_local
                )
            
            logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
            try:
                self._model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    **model_kwargs
                )
            except Exception as model_error:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {model_error}")
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å, –Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω, 
                # —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ None –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è fallback
                logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback —Ä–µ–∂–∏–º")
                self._model = None
                if self._tokenizer:
                    if self._tokenizer.pad_token is None:
                        self._tokenizer.pad_token = self._tokenizer.eos_token
                return  # –í—ã—Ö–æ–¥–∏–º, –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç None, –Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–∞–∫–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
            if device == "cuda":
                # –ü—Ä–∏ device_map="auto" –º–æ–¥–µ–ª—å —É–∂–µ –Ω–∞ GPU, –ø—Ä–æ–≤–µ—Ä—è–µ–º
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–µ—Ä–≤–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –º–æ–¥–µ–ª–∏
                    first_param = next(self._model.parameters())
                    actual_device = first_param.device
                    logger.info(f"üîç –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {actual_device}")
                    
                    if actual_device.type != "cuda":
                        logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞ GPU! –¢–µ–∫—É—â–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {actual_device}, –ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ cuda...")
                        self._model = self._model.to("cuda")
                        logger.info("‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ cuda")
                    else:
                        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —É–∂–µ –Ω–∞ GPU: {actual_device}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: {e}, –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –Ω–∞ cuda...")
                    try:
                        self._model = self._model.to("cuda")
                        logger.info("‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ cuda")
                    except Exception as move_error:
                        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ cuda: {move_error}")
            elif device == "cpu":
                self._model = self._model.to("cpu")
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞ CPU")
            
            self._model.eval()  # Set to evaluation mode
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
            try:
                final_device = next(self._model.parameters()).device
                logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {final_device}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {e}")
            
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å Qwen —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {device}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ None –¥–ª—è fallback —Ä–µ–∂–∏–º–∞
            self._model = None
            if self._tokenizer and self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
            logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback —Ä–µ–∂–∏–º (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º)")
    
    def _get_best_device(self) -> str:
        """Get best available device"""
        # Check if device is forced via settings
        if settings.QWEN_DEVICE and settings.QWEN_DEVICE.lower() != "auto":
            device = settings.QWEN_DEVICE.lower()
            if device == "cpu":
                return "cpu"
            elif device == "cuda" and torch.cuda.is_available():
                return "cuda"
            elif device == "mps" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                return "mps"
            else:
                logger.warning(f"Requested device '{device}' not available, falling back to CPU")
                return "cpu"
        
        # Auto-detect best device
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    
    async def classify_metrics_from_rag(
        self,
        metrics: Dict[str, any]
    ) -> Dict[str, Any]:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –æ—Ç RAG
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: RAG –ø–µ—Ä–µ–¥–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ ‚Üí Qwen –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç ‚Üí —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        
        Args:
            metrics: –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –æ—Ç RAG
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è RAG ‚Üí Postgres)
        """
        text = metrics.get("text", "")
        filename = metrics.get("filename", "")
        
        # Try to load model, but use fallback if it fails
        try:
            self._ensure_model_loaded()
        except Exception as e:
            logger.warning(f"Failed to load Qwen model, using fallback classification: {e}")
            return {
                "classification": self._fallback_classify(text, filename),
                "processed": False,
                "error": f"Model not available: {str(e)}",
                "chunks_count": metrics.get("chunks_count", 0),
                "text_length": metrics.get("text_length", 0)
            }
        
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏:
1. –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (contract, invoice, act, order, email, scan, document, presentation, report)
2. –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏-–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
3. –î–∞—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
4. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (high, medium, low)
5. –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
6. –¢–µ–≥–∏ - –≤—ã–¥–µ–ª–∏ 3-7 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤/—Ñ—Ä–∞–∑, –∫–æ—Ç–æ—Ä—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—Ç –¥–æ–∫—É–º–µ–Ω—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–¥–æ–≥–æ–≤–æ—Ä", "–ø–æ—Å—Ç–∞–≤–∫–∞", "2024", "–û–û–û –†–æ–≥–∞", "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è")

–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:
{text[:2000]}

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "type": "—Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞",
    "counterparty_name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ null",
    "date": "YYYY-MM-DD –∏–ª–∏ null",
    "priority": "high/medium/low",
    "description": "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ",
    "tags": ["—Ç–µ–≥1", "—Ç–µ–≥2", "—Ç–µ–≥3"]
}}"""
        
        try:
            logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è {filename}...")
            import asyncio
            import signal
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            try:
                response = await asyncio.wait_for(
                    asyncio.to_thread(
                        self._generate_text,
                        prompt=prompt,
                        max_new_tokens=256,
                        temperature=0.3
                    ),
                    timeout=60.0  # 60 —Å–µ–∫—É–Ω–¥ —Ç–∞–π–º–∞—É—Ç
                )
                logger.info(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è {filename}")
            except asyncio.TimeoutError:
                logger.error(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è {filename} (>60 —Å–µ–∫), –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                classification = self._fallback_classify(text, filename)
                return {
                    "classification": classification,
                    "processed": False,
                    "error": "Generation timeout",
                    "chunks_count": metrics.get("chunks_count", 0),
                    "text_length": metrics.get("text_length", 0)
                }
            
            # Parse JSON from response
            import re
            # –ò—â–µ–º JSON –æ–±—ä–µ–∫—Ç –≤ –æ—Ç–≤–µ—Ç–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–º)
            json_match = re.search(r'\{.*?"tags".*?\}', response, re.DOTALL)
            if not json_match:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±–æ–π JSON –æ–±—ä–µ–∫—Ç
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            
            if json_match:
                try:
                    classification = json.loads(json_match.group())
                    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ tags - —ç—Ç–æ —Å–ø–∏—Å–æ–∫
                    if "tags" in classification and not isinstance(classification["tags"], list):
                        if isinstance(classification["tags"], str):
                            # –ï—Å–ª–∏ —Ç–µ–≥–∏ –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º
                            classification["tags"] = [t.strip() for t in classification["tags"].split(",") if t.strip()]
                        else:
                            classification["tags"] = []
                    elif "tags" not in classification:
                        classification["tags"] = []
                except json.JSONDecodeError as e:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –æ—Ç–≤–µ—Ç: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    classification = self._fallback_classify(text, filename)
            else:
                classification = self._fallback_classify(text, filename)
                classification["tags"] = []
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è RAG ‚Üí Postgres
            reverse_metrics = {
                "classification": classification,
                "processed": True,
                "chunks_count": metrics.get("chunks_count", 0),
                "text_length": metrics.get("text_length", 0)
            }
            
            logger.info(f"‚úÖ Qwen –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª –¥–æ–∫—É–º–µ–Ω—Ç {filename}, —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
            return reverse_metrics
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫: {e}")
            return {
                "classification": self._fallback_classify(text, filename),
                "processed": False,
                "error": str(e)
            }
    
    async def save_document_to_redis(
        self,
        document_id: str,
        file_data: bytes,
        metadata: Dict[str, any]
    ):
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ Redis
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: Qwen ‚Üí –¥–æ–∫—É–º–µ–Ω—Ç—ã ‚Üí Redis
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            file_data: –î–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        try:
            from app.core.redis_client import get_redis
            import base64
            
            redis = await get_redis()
            
            # –ö–æ–¥–∏—Ä—É–µ–º —Ñ–∞–π–ª –≤ base64 –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Redis
            file_base64 = base64.b64encode(file_data).decode('utf-8')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            document_key = f"document:{document_id}"
            await redis.setex(
                document_key,
                86400 * 7,  # 7 –¥–Ω–µ–π
                json.dumps({
                    "data": file_base64,
                    "metadata": metadata,
                    "size": len(file_data)
                })
            )
            
            logger.info(f"‚úÖ Qwen —Å–æ—Ö—Ä–∞–Ω–∏–ª –¥–æ–∫—É–º–µ–Ω—Ç {document_id} –≤ Redis")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Redis: {e}")
            raise
    
    async def get_document_from_redis(
        self,
        document_id: str
    ) -> Optional[Dict[str, any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ Redis
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: Qwen ‚Üí –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –≤ Redis ‚Üí –ø–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            –î–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None
        """
        try:
            from app.core.redis_client import get_redis
            import base64
            
            redis = await get_redis()
            
            document_key = f"document:{document_id}"
            data = await redis.get(document_key)
            
            if data:
                document_data = json.loads(data)
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ñ–∞–π–ª
                file_data = base64.b64decode(document_data["data"])
                return {
                    "data": file_data,
                    "metadata": document_data.get("metadata", {}),
                    "size": document_data.get("size", 0)
                }
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ Redis: {e}")
            return None
    
    async def process_search_query(
        self,
        query: str,
        rag_service,
        db
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: Qwen ‚Üí RAG ‚Üí Postgres ‚Üí Qwen ‚Üí Redis ‚Üí –æ—Ç–≤–µ—Ç
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            rag_service: –≠–∫–∑–µ–º–ø–ª—è—Ä RAG —Å–µ—Ä–≤–∏—Å–∞
            db: Database session
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        """
        try:
            # Qwen –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ RAG
            logger.info(f"Qwen –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {query}")
            logger.info(f"üîç Qwen ‚Üí RAG ‚Üí Postgres: –Ω–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            
            # RAG –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ Postgres - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º top_k –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            chunks = await rag_service.search_for_qwen(db, query, top_k=30)
            logger.info(f"‚úÖ RAG ‚Üí Postgres: –Ω–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            if not chunks:
                return {
                    "answer": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.",
                    "documents": [],
                    "chunks": []
                }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ similarity –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö
            sorted_chunks = sorted(chunks, key=lambda x: x.get('similarity', 0.0), reverse=True)
            
            context = "\n\n".join([
                f"–î–æ–∫—É–º–µ–Ω—Ç: {chunk['document_title']} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {chunk.get('similarity', 0.0):.3f})\n{chunk['text'][:400]}"
                for chunk in sorted_chunks[:10]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ø-10 –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            ])
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–ê–ñ–ù–û: 
- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –î–ï–ô–°–¢–í–ò–¢–ï–õ–¨–ù–û –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –ò–≥–Ω–æ—Ä–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –∏–º–µ—é—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏—è –∫ –∑–∞–ø—Ä–æ—Å—É, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
- –ï—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, —Å–∫–∞–∂–∏ —á—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏):
{context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ —Ç–æ—á–Ω–æ. –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∑–∞–ø—Ä–æ—Å—É, –ù–ï —É–ø–æ–º–∏–Ω–∞–π –µ–≥–æ.
–ï—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç, —Å–∫–∞–∂–∏ "–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"."""
            
            answer = self._generate_text(
                prompt=prompt,
                max_new_tokens=256,
                temperature=0.7
            )
            
            # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ –±–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é similarity –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            seen_doc_ids = {}
            documents = []
            
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —á–∞–Ω–∫–∞–º –∏ —Å–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π similarity
            for chunk in sorted_chunks:  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
                doc_id = chunk["document_id"]
                similarity = chunk.get("similarity", 0.0)
                
                # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω, –æ–±–Ω–æ–≤–ª—è–µ–º similarity –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω
                if doc_id in seen_doc_ids:
                    if similarity > seen_doc_ids[doc_id].get("similarity", 0.0):
                        seen_doc_ids[doc_id]["similarity"] = similarity
                    continue
                
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ Redis
                logger.debug(f"üîç Qwen ‚Üí Redis: –ø—Ä–æ–≤–µ—Ä—è—é –¥–æ–∫—É–º–µ–Ω—Ç {doc_id}")
                doc_data = await self.get_document_from_redis(doc_id)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π similarity
                doc_info = {
                    "document_id": doc_id,
                    "title": chunk["document_title"],
                    "type": chunk["document_type"],
                    "path": chunk.get("document_path"),
                    "available": doc_data is not None,
                    "similarity": similarity
                }
                seen_doc_ids[doc_id] = doc_info
                documents.append(doc_info)
                
                if doc_data:
                    logger.debug(f"‚úÖ Qwen ‚Üí Redis: –¥–æ–∫—É–º–µ–Ω—Ç {doc_id} –Ω–∞–π–¥–µ–Ω –≤ Redis")
                else:
                    logger.debug(f"‚ö†Ô∏è Qwen ‚Üí Redis: –¥–æ–∫—É–º–µ–Ω—Ç {doc_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ Redis (–∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Postgres)")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ similarity (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏) - –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–µ—Ä–≤—ã–º–∏
            documents.sort(key=lambda x: x.get("similarity", 0.0), reverse=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ—á–µ–Ω—å –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ (0.85) –∏–∑-–∑–∞ –≤—ã—Å–æ–∫–æ–π similarity –º–µ–∂–¥—É –≤—Å–µ–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            # –¢–∞–∫–∂–µ –ª–æ–≥–∏—Ä—É–µ–º similarity –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(documents)}")
            for doc in documents:
                logger.info(f"  - {doc['title']}: similarity={doc.get('similarity', 0.0):.3f}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –∏ –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø –¥–æ–∫—É–º–µ–Ω—Ç—ã
            # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å similarity > 0.9, –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∏—Ö
            high_relevance = [doc for doc in documents if doc.get("similarity", 0.0) >= 0.90]
            if high_relevance:
                filtered_documents = high_relevance
                logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é (similarity >= 0.90): {len(filtered_documents)}")
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –æ—á–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö, –±–µ—Ä–µ–º —Ç–æ–ø-1 –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π
                filtered_documents = documents[:1] if documents else []
                logger.info(f"‚ö†Ô∏è –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å similarity >= 0.90, –±–µ—Ä–µ–º —Ç–æ–ø-1: {len(filtered_documents)}")
            
            documents = filtered_documents
            
            logger.info(f"‚úÖ Qwen —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å, –Ω–∞–π–¥–µ–Ω–æ {len(documents)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            return {
                "answer": answer,
                "documents": documents,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                "chunks": chunks,
                "query": query
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return {
                "answer": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}",
                "documents": [],
                "chunks": []
            }
    
    def classify_document(self, text: str, filename: str = "") -> Dict[str, Any]:
        """Legacy method for direct classification - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç fallback –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        # –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ classify_metrics_from_rag
        return self._fallback_classify(text, filename)
    
    def _generate_text(
        self,
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """Generate text using Qwen model"""
        if self._model is None or self._tokenizer is None:
            raise RuntimeError("Model not loaded")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –º–æ–¥–µ–ª—å (GPU –∏–ª–∏ CPU)
            device = next(self._model.parameters()).device
            logger.info(f"üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {device}")
            logger.info(f"üìù –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤, max_new_tokens: {max_new_tokens}")
            
            inputs = self._tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            )
            
            # Inputs –Ω–∞ —Ç–æ–º –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ —á—Ç–æ –∏ –º–æ–¥–µ–ª—å
            inputs = {k: v.to(device) for k, v in inputs.items()}
            logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞—é generate() –Ω–∞ {device}...")
            
            with torch.no_grad():
                outputs = self._model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self._tokenizer.pad_token_id,
                    eos_token_id=self._tokenizer.eos_token_id,
                    repetition_penalty=1.2
                )
            
            logger.info(f"‚úÖ generate() –∑–∞–≤–µ—Ä—à–µ–Ω, –¥–ª–∏–Ω–∞ –≤—ã–≤–æ–¥–∞: {outputs.shape}")
            
            generated_text = self._tokenizer.decode(
                outputs[0],
                skip_special_tokens=True
            )
            
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
            if generated_text.startswith(prompt):
                generated_text = generated_text[len(prompt):].strip()
            
            # –£–±–∏—Ä–∞–µ–º "–¥—É–º–∞—é—â–∏–π" —Ä–µ–∂–∏–º Qwen3 (—Ç–µ–≥–∏ <think>)
            import re
            generated_text = re.sub(r'<think>.*?</think>', '', generated_text, flags=re.DOTALL)
            generated_text = re.sub(r'<\|.*?\|>', '', generated_text)  # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü. —Ç–æ–∫–µ–Ω—ã
            
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
            lines = generated_text.strip().split('\n')
            if lines:
                # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—É—é –Ω–µ–ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('Answer:') and not line.startswith('–û—Ç–≤–µ—Ç:'):
                        generated_text = line
                        break
            
            return generated_text.strip()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            raise
    
    def _fallback_classify(self, text: str, filename: str) -> Dict[str, Any]:
        """Fallback classification based on keywords"""
        text_lower = text.lower()
        filename_lower = filename.lower()
        
        doc_type = "scan"
        if any(word in text_lower for word in ["–¥–æ–≥–æ–≤–æ—Ä", "–∫–æ–Ω—Ç—Ä–∞–∫—Ç", "—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ"]):
            doc_type = "contract"
        elif any(word in text_lower for word in ["—Å—á–µ—Ç", "invoice", "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞"]):
            doc_type = "invoice"
        elif any(word in text_lower for word in ["–∞–∫—Ç", "–ø—Ä–∏–µ–º–∫–∏", "–≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"]):
            doc_type = "act"
        elif any(word in text_lower for word in ["–ø—Ä–∏–∫–∞–∑", "—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ", "order"]):
            doc_type = "order"
        elif any(word in text_lower for word in ["–ø–∏—Å—å–º–æ", "email", "—Å–æ–æ–±—â–µ–Ω–∏–µ"]):
            doc_type = "email"
        
        priority = "medium"
        if any(word in text_lower for word in ["—Å—Ä–æ—á–Ω–æ", "urgent", "–≤–∞–∂–Ω–æ", "important"]):
            priority = "high"
        elif any(word in text_lower for word in ["–Ω–∏–∑–∫–∏–π", "low", "–Ω–µ–≤–∞–∂–Ω–æ"]):
            priority = "low"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Ç–µ–≥–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
        from pathlib import Path
        fallback_tags = []
        if filename:
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∫–∞–∫ —Ç–µ–≥
            file_ext = Path(filename).suffix.lower()
            if file_ext:
                fallback_tags.append(file_ext.replace('.', ''))
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∫–∞–∫ —Ç–µ–≥
        if doc_type != "scan":
            fallback_tags.append(doc_type)
        
        return {
            "type": doc_type,
            "counterparty_name": None,
            "date": None,
            "priority": priority,
            "description": f"–î–æ–∫—É–º–µ–Ω—Ç: {filename or '–±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'}",
            "tags": fallback_tags
        }


# Singleton instance - —Å–æ–∑–¥–∞–µ—Ç—Å—è –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ, –Ω–æ –º–æ–¥–µ–ª—å –ù–ï –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è
# –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ _ensure_model_loaded()
qwen_service = QwenService()
logger.debug("QwenService singleton created (model not loaded yet - lazy loading)")
