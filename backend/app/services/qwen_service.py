"""
Qwen model service for document classification
–°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ:
- –ü—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: –ø–æ–ª—É—á–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç RAG, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ Redis
- –ü—Ä–∏ –ø–æ–∏—Å–∫–µ: –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ RAG/Postgres, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç, –ø–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Redis
"""
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Optional, Dict, Any
import logging
import os
import json
from app.core.config import settings

logger = logging.getLogger(__name__)


class QwenService:
    """Service for Qwen model operations"""
    
    _instance = None
    _model = None
    _tokenizer = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(QwenService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        # Lazy loading - model will be loaded on first use
        # This prevents blocking startup if model download is needed
        pass
    
    def _ensure_model_loaded(self):
        """Ensure model is loaded (lazy loading) - –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏"""
        if self._model is None or self._tokenizer is None:
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Qwen –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏ (lazy loading, –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å)...")
            try:
                self._load_model()
            except Exception as e:
                logger.error(f"‚ùå Failed to load Qwen model: {e}", exc_info=True)
                raise
    
    def _load_model(self):
        """Load Qwen model - –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ (lazy loading)"""
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ Qwen (lazy loading)...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Qwen3-4B –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –ø—Ä–æ–µ–∫—Ç–µ)
        model_name = settings.QWEN_MODEL_NAME
        use_local = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (—Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤, –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏)
        if settings.QWEN_MODEL_PATH:
            model_path = settings.QWEN_MODEL_PATH
            index_file = os.path.join(model_path, "model.safetensors.index.json")
            
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ —á—Ç–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
            if os.path.isdir(model_path) and os.path.isfile(index_file):
                try:
                    # –¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤, –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –∏—Ö
                    with open(index_file, 'r') as f:
                        index_data = json.load(f)
                    
                    weight_map = index_data.get('weight_map', {})
                    required_files = set(weight_map.values())
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤, –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ–º
                    all_files_exist = all(
                        os.path.isfile(os.path.join(model_path, fname))
                        for fname in required_files
                    )
                    
                    if all_files_exist:
                        use_local = True
                        model_name = model_path
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {model_path}, –Ω–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É...")
                    else:
                        missing = [f for f in required_files if not os.path.isfile(os.path.join(model_path, f))]
                        logger.warning(f"‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–ø–æ–ª–Ω–∞—è, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã: {missing[:5]}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            else:
                logger.info(f"üì• –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {model_path}, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ Hugging Face")
        
        if not use_local:
            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face: {model_name}")
            logger.warning("‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ...")
        
        device = self._get_best_device()
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {model_name} –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {device}")
        
        model_kwargs = {
            "dtype": torch.float32,  # Always use float32 for CPU compatibility
            "device_map": None,  # Explicitly set to None for CPU
            "trust_remote_code": True
        }
        
        # Quantization –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ (–æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è Mac)
        if settings.QWEN_LOAD_IN_8BIT:
            model_kwargs["load_in_8bit"] = True
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 8-bit quantization")
        elif settings.QWEN_LOAD_IN_4BIT:
            model_kwargs["load_in_4bit"] = True
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 4-bit quantization (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Mac)")
        
        try:
            logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
            # –î–ª—è Qwen3 –∏—Å–ø–æ–ª—å–∑—É–µ–º Qwen2Tokenizer (Qwen3 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ—Ç –∂–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä)
            try:
                from transformers import Qwen2Tokenizer
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º Qwen2Tokenizer –¥–ª—è Qwen3 –º–æ–¥–µ–ª–∏...")
                self._tokenizer = Qwen2Tokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
            except (ImportError, Exception) as tokenizer_error:
                logger.warning(f"‚ö†Ô∏è Qwen2Tokenizer –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({tokenizer_error}), –ø—Ä–æ–±—É–µ–º AutoTokenizer...")
                # Fallback –Ω–∞ AutoTokenizer
                self._tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
            
            logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
            try:
                self._model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    **model_kwargs
                )
            except Exception as model_error:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {model_error}")
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å, –Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω, 
                # —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ None –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è fallback
                logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback —Ä–µ–∂–∏–º")
                self._model = None
                if self._tokenizer:
                    if self._tokenizer.pad_token is None:
                        self._tokenizer.pad_token = self._tokenizer.eos_token
                return  # –í—ã—Ö–æ–¥–∏–º, –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç None, –Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω
            
            # Explicitly move model to device if CPU
            if device == "cpu":
                self._model = self._model.to("cpu")
            self._model.eval()  # Set to evaluation mode
            
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å Qwen —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {device}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ None –¥–ª—è fallback —Ä–µ–∂–∏–º–∞
            self._model = None
            if self._tokenizer and self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
            logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback —Ä–µ–∂–∏–º (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º)")
    
    def _get_best_device(self) -> str:
        """Get best available device"""
        # Check if device is forced via settings
        if settings.QWEN_DEVICE and settings.QWEN_DEVICE.lower() != "auto":
            device = settings.QWEN_DEVICE.lower()
            if device == "cpu":
                return "cpu"
            elif device == "cuda" and torch.cuda.is_available():
                return "cuda"
            elif device == "mps" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                return "mps"
            else:
                logger.warning(f"Requested device '{device}' not available, falling back to CPU")
                return "cpu"
        
        # Auto-detect best device
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    
    async def classify_metrics_from_rag(
        self,
        metrics: Dict[str, any]
    ) -> Dict[str, Any]:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –æ—Ç RAG
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: RAG –ø–µ—Ä–µ–¥–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ ‚Üí Qwen –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç ‚Üí —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        
        Args:
            metrics: –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –æ—Ç RAG
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è RAG ‚Üí Postgres)
        """
        text = metrics.get("text", "")
        filename = metrics.get("filename", "")
        
        # Try to load model, but use fallback if it fails
        try:
            self._ensure_model_loaded()
        except Exception as e:
            logger.warning(f"Failed to load Qwen model, using fallback classification: {e}")
            return {
                "classification": self._fallback_classify(text, filename),
                "processed": False,
                "error": f"Model not available: {str(e)}",
                "chunks_count": metrics.get("chunks_count", 0),
                "text_length": metrics.get("text_length", 0)
            }
        
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏:
1. –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (contract, invoice, act, order, email, scan)
2. –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏-–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
3. –î–∞—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
4. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (high, medium, low)
5. –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:
{text[:2000]}

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "type": "—Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞",
    "counterparty_name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ null",
    "date": "YYYY-MM-DD –∏–ª–∏ null",
    "priority": "high/medium/low",
    "description": "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ"
}}"""
        
        try:
            response = self._generate_text(
                prompt=prompt,
                max_new_tokens=256,
                temperature=0.3
            )
            
            # Parse JSON from response
            import re
            json_match = re.search(r'\{[^}]+\}', response, re.DOTALL)
            if json_match:
                classification = json.loads(json_match.group())
            else:
                classification = self._fallback_classify(text, filename)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è RAG ‚Üí Postgres
            reverse_metrics = {
                "classification": classification,
                "processed": True,
                "chunks_count": metrics.get("chunks_count", 0),
                "text_length": metrics.get("text_length", 0)
            }
            
            logger.info(f"‚úÖ Qwen –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª –¥–æ–∫—É–º–µ–Ω—Ç {filename}, —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
            return reverse_metrics
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫: {e}")
            return {
                "classification": self._fallback_classify(text, filename),
                "processed": False,
                "error": str(e)
            }
    
    async def save_document_to_redis(
        self,
        document_id: str,
        file_data: bytes,
        metadata: Dict[str, any]
    ):
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ Redis
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: Qwen ‚Üí –¥–æ–∫—É–º–µ–Ω—Ç—ã ‚Üí Redis
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            file_data: –î–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        try:
            from app.core.redis_client import get_redis
            import base64
            
            redis = await get_redis()
            
            # –ö–æ–¥–∏—Ä—É–µ–º —Ñ–∞–π–ª –≤ base64 –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Redis
            file_base64 = base64.b64encode(file_data).decode('utf-8')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            document_key = f"document:{document_id}"
            await redis.setex(
                document_key,
                86400 * 7,  # 7 –¥–Ω–µ–π
                json.dumps({
                    "data": file_base64,
                    "metadata": metadata,
                    "size": len(file_data)
                })
            )
            
            logger.info(f"‚úÖ Qwen —Å–æ—Ö—Ä–∞–Ω–∏–ª –¥–æ–∫—É–º–µ–Ω—Ç {document_id} –≤ Redis")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Redis: {e}")
            raise
    
    async def get_document_from_redis(
        self,
        document_id: str
    ) -> Optional[Dict[str, any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ Redis
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: Qwen ‚Üí –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –≤ Redis ‚Üí –ø–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            –î–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None
        """
        try:
            from app.core.redis_client import get_redis
            import base64
            
            redis = await get_redis()
            
            document_key = f"document:{document_id}"
            data = await redis.get(document_key)
            
            if data:
                document_data = json.loads(data)
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ñ–∞–π–ª
                file_data = base64.b64decode(document_data["data"])
                return {
                    "data": file_data,
                    "metadata": document_data.get("metadata", {}),
                    "size": document_data.get("size", 0)
                }
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ Redis: {e}")
            return None
    
    async def process_search_query(
        self,
        query: str,
        rag_service,
        db
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: Qwen ‚Üí RAG ‚Üí Postgres ‚Üí Qwen ‚Üí Redis ‚Üí –æ—Ç–≤–µ—Ç
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            rag_service: –≠–∫–∑–µ–º–ø–ª—è—Ä RAG —Å–µ—Ä–≤–∏—Å–∞
            db: Database session
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        """
        try:
            # Qwen –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ RAG
            logger.info(f"Qwen –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {query}")
            logger.info(f"üîç Qwen ‚Üí RAG ‚Üí Postgres: –Ω–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            
            # RAG –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ Postgres - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º top_k –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            chunks = await rag_service.search_for_qwen(db, query, top_k=20)
            logger.info(f"‚úÖ RAG ‚Üí Postgres: –Ω–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            if not chunks:
                return {
                    "answer": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.",
                    "documents": [],
                    "chunks": []
                }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
            context = "\n\n".join([
                f"–î–æ–∫—É–º–µ–Ω—Ç: {chunk['document_title']}\n{chunk['text'][:300]}"
                for chunk in chunks[:10]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ø-10 —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            ])
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {query}

–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."""
            
            answer = self._generate_text(
                prompt=prompt,
                max_new_tokens=256,
                temperature=0.7
            )
            
            # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –≤—Å–µ—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            seen_doc_ids = set()
            documents = []
            
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —á–∞–Ω–∫–∞–º –∏ —Å–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            for chunk in chunks:
                doc_id = chunk["document_id"]
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —É–∂–µ –¥–æ–±–∞–≤–∏–ª–∏ —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç
                if doc_id in seen_doc_ids:
                    continue
                
                seen_doc_ids.add(doc_id)
                
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ Redis
                logger.debug(f"üîç Qwen ‚Üí Redis: –ø—Ä–æ–≤–µ—Ä—è—é –¥–æ–∫—É–º–µ–Ω—Ç {doc_id}")
                doc_data = await self.get_document_from_redis(doc_id)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç (–¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ—Ç –≤ Redis, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —á–∞–Ω–∫–∞)
                documents.append({
                    "document_id": doc_id,
                    "title": chunk["document_title"],
                    "type": chunk["document_type"],
                    "path": chunk.get("document_path"),
                    "available": doc_data is not None,
                    "similarity": chunk.get("similarity", 0.0)  # –î–æ–±–∞–≤–ª—è–µ–º similarity –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
                })
                if doc_data:
                    logger.debug(f"‚úÖ Qwen ‚Üí Redis: –¥–æ–∫—É–º–µ–Ω—Ç {doc_id} –Ω–∞–π–¥–µ–Ω –≤ Redis")
                else:
                    logger.debug(f"‚ö†Ô∏è Qwen ‚Üí Redis: –¥–æ–∫—É–º–µ–Ω—Ç {doc_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ Redis (–∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Postgres)")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ similarity (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏)
            documents.sort(key=lambda x: x.get("similarity", 0.0), reverse=True)
            
            logger.info(f"‚úÖ Qwen —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å, –Ω–∞–π–¥–µ–Ω–æ {len(documents)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            return {
                "answer": answer,
                "documents": documents,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                "chunks": chunks,
                "query": query
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return {
                "answer": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}",
                "documents": [],
                "chunks": []
            }
    
    def classify_document(self, text: str, filename: str = "") -> Dict[str, Any]:
        """Legacy method for direct classification"""
        try:
            self._ensure_model_loaded()
            return self._fallback_classify(text, filename)
        except Exception as e:
            logger.warning(f"Model not available, using fallback: {e}")
            return self._fallback_classify(text, filename)
    
    def _generate_text(
        self,
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """Generate text using Qwen model"""
        if self._model is None or self._tokenizer is None:
            raise RuntimeError("Model not loaded")
        
        try:
            # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å MPS
            original_device = next(self._model.parameters()).device
            model_on_cpu = self._model.to("cpu")
            
            inputs = self._tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            )
            
            # Inputs –≤—Å–µ–≥–¥–∞ –Ω–∞ CPU
            inputs = {k: v.to("cpu") for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model_on_cpu.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self._tokenizer.pad_token_id,
                    eos_token_id=self._tokenizer.eos_token_id
                )
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self._model = model_on_cpu.to(original_device)
            
            generated_text = self._tokenizer.decode(
                outputs[0],
                skip_special_tokens=True
            )
            
            if generated_text.startswith(prompt):
                generated_text = generated_text[len(prompt):].strip()
            
            return generated_text
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            raise
    
    def _fallback_classify(self, text: str, filename: str) -> Dict[str, Any]:
        """Fallback classification based on keywords"""
        text_lower = text.lower()
        filename_lower = filename.lower()
        
        doc_type = "scan"
        if any(word in text_lower for word in ["–¥–æ–≥–æ–≤–æ—Ä", "–∫–æ–Ω—Ç—Ä–∞–∫—Ç", "—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ"]):
            doc_type = "contract"
        elif any(word in text_lower for word in ["—Å—á–µ—Ç", "invoice", "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞"]):
            doc_type = "invoice"
        elif any(word in text_lower for word in ["–∞–∫—Ç", "–ø—Ä–∏–µ–º–∫–∏", "–≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"]):
            doc_type = "act"
        elif any(word in text_lower for word in ["–ø—Ä–∏–∫–∞–∑", "—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ", "order"]):
            doc_type = "order"
        elif any(word in text_lower for word in ["–ø–∏—Å—å–º–æ", "email", "—Å–æ–æ–±—â–µ–Ω–∏–µ"]):
            doc_type = "email"
        
        priority = "medium"
        if any(word in text_lower for word in ["—Å—Ä–æ—á–Ω–æ", "urgent", "–≤–∞–∂–Ω–æ", "important"]):
            priority = "high"
        elif any(word in text_lower for word in ["–Ω–∏–∑–∫–∏–π", "low", "–Ω–µ–≤–∞–∂–Ω–æ"]):
            priority = "low"
        
        return {
            "type": doc_type,
            "counterparty_name": None,
            "date": None,
            "priority": priority,
            "description": f"–î–æ–∫—É–º–µ–Ω—Ç: {filename or '–±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'}"
        }


# Singleton instance - —Å–æ–∑–¥–∞–µ—Ç—Å—è –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ, –Ω–æ –º–æ–¥–µ–ª—å –ù–ï –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è
# –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ _ensure_model_loaded()
qwen_service = QwenService()
logger.debug("QwenService singleton created (model not loaded yet - lazy loading)")
