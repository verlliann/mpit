"""
Qwen model service for document classification
–°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ:
- –ü—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: –ø–æ–ª—É—á–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç RAG, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ Redis
- –ü—Ä–∏ –ø–æ–∏—Å–∫–µ: –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ RAG/Postgres, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç, –ø–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Redis
"""
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Optional, Dict, Any
import logging
import os
import json
from app.core.config import settings

logger = logging.getLogger(__name__)


class QwenService:
    """Service for Qwen model operations"""
    
    _instance = None
    _model = None
    _tokenizer = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(QwenService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        # Lazy loading - model will be loaded on first use
        # This prevents blocking startup if model download is needed
        pass
    
    def _ensure_model_loaded(self):
        """Ensure model is loaded (lazy loading)"""
        if self._model is None or self._tokenizer is None:
            logger.info("Loading Qwen model (lazy loading)...")
            try:
                self._load_model()
            except Exception as e:
                logger.error(f"Failed to load Qwen model: {e}", exc_info=True)
                raise
    
    def _load_model(self):
        """Load Qwen model"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        model_name = "Qwen/Qwen2.5-0.5B-Instruct"  # Fallback –Ω–∞ Hugging Face
        use_local = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        if settings.QWEN_MODEL_PATH:
            model_path = settings.QWEN_MODEL_PATH
            index_file = os.path.join(model_path, "model.safetensors.index.json")
            
            if os.path.exists(model_path) and os.path.exists(index_file):
                try:
                    with open(index_file, 'r') as f:
                        index_data = json.load(f)
                    
                    weight_map = index_data.get('weight_map', {})
                    required_files = set(weight_map.values())
                    all_files_exist = all(
                        os.path.exists(os.path.join(model_path, fname))
                        for fname in required_files
                    )
                    
                    if all_files_exist:
                        use_local = True
                        model_name = model_path
                        logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {model_path}")
                    else:
                        missing = [f for f in required_files if not os.path.exists(os.path.join(model_path, f))]
                        logger.warning(f"‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–ø–æ–ª–Ω–∞—è, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã: {missing[:5]}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            else:
                logger.info(f"üì• –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {model_path}, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ Hugging Face")
        
        if not use_local:
            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face: {model_name}")
        
        device = self._get_best_device()
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {model_name} –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {device}")
        
        model_kwargs = {
            "dtype": torch.float32,  # Always use float32 for CPU compatibility
            "device_map": None,  # Explicitly set to None for CPU
            "trust_remote_code": True
        }
        
        # Quantization –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ (–æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è Mac)
        if settings.QWEN_LOAD_IN_8BIT:
            model_kwargs["load_in_8bit"] = True
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 8-bit quantization")
        elif settings.QWEN_LOAD_IN_4BIT:
            model_kwargs["load_in_4bit"] = True
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 4-bit quantization (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Mac)")
        
        try:
            self._tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            self._model = AutoModelForCausalLM.from_pretrained(
                model_name,
                **model_kwargs
            )
            
            # Explicitly move model to device if CPU
            if device == "cpu":
                self._model = self._model.to("cpu")
                self._model.eval()  # Set to evaluation mode
            
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å Qwen —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {device}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def _get_best_device(self) -> str:
        """Get best available device"""
        # Check if device is forced via settings
        if settings.QWEN_DEVICE and settings.QWEN_DEVICE.lower() != "auto":
            device = settings.QWEN_DEVICE.lower()
            if device == "cpu":
                return "cpu"
            elif device == "cuda" and torch.cuda.is_available():
                return "cuda"
            elif device == "mps" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                return "mps"
            else:
                logger.warning(f"Requested device '{device}' not available, falling back to CPU")
                return "cpu"
        
        # Auto-detect best device
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    
    async def classify_metrics_from_rag(
        self,
        metrics: Dict[str, any]
    ) -> Dict[str, Any]:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –æ—Ç RAG
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: RAG –ø–µ—Ä–µ–¥–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ ‚Üí Qwen –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç ‚Üí —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        
        Args:
            metrics: –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –æ—Ç RAG
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è RAG ‚Üí Postgres)
        """
        text = metrics.get("text", "")
        filename = metrics.get("filename", "")
        
        # Try to load model, but use fallback if it fails
        try:
            self._ensure_model_loaded()
        except Exception as e:
            logger.warning(f"Failed to load Qwen model, using fallback classification: {e}")
            return {
                "classification": self._fallback_classify(text, filename),
                "processed": False,
                "error": f"Model not available: {str(e)}",
                "chunks_count": metrics.get("chunks_count", 0),
                "text_length": metrics.get("text_length", 0)
            }
        
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏:
1. –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (contract, invoice, act, order, email, scan)
2. –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏-–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
3. –î–∞—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
4. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (high, medium, low)
5. –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:
{text[:2000]}

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "type": "—Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞",
    "counterparty_name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ null",
    "date": "YYYY-MM-DD –∏–ª–∏ null",
    "priority": "high/medium/low",
    "description": "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ"
}}"""
        
        try:
            response = self._generate_text(
                prompt=prompt,
                max_new_tokens=256,
                temperature=0.3
            )
            
            # Parse JSON from response
            import re
            json_match = re.search(r'\{[^}]+\}', response, re.DOTALL)
            if json_match:
                classification = json.loads(json_match.group())
            else:
                classification = self._fallback_classify(text, filename)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è RAG ‚Üí Postgres
            reverse_metrics = {
                "classification": classification,
                "processed": True,
                "chunks_count": metrics.get("chunks_count", 0),
                "text_length": metrics.get("text_length", 0)
            }
            
            logger.info(f"‚úÖ Qwen –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª –¥–æ–∫—É–º–µ–Ω—Ç {filename}, —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
            return reverse_metrics
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫: {e}")
            return {
                "classification": self._fallback_classify(text, filename),
                "processed": False,
                "error": str(e)
            }
    
    async def save_document_to_redis(
        self,
        document_id: str,
        file_data: bytes,
        metadata: Dict[str, any]
    ):
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ Redis
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: Qwen ‚Üí –¥–æ–∫—É–º–µ–Ω—Ç—ã ‚Üí Redis
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            file_data: –î–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        try:
            from app.core.redis_client import get_redis
            import base64
            
            redis = await get_redis()
            
            # –ö–æ–¥–∏—Ä—É–µ–º —Ñ–∞–π–ª –≤ base64 –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Redis
            file_base64 = base64.b64encode(file_data).decode('utf-8')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            document_key = f"document:{document_id}"
            await redis.setex(
                document_key,
                86400 * 7,  # 7 –¥–Ω–µ–π
                json.dumps({
                    "data": file_base64,
                    "metadata": metadata,
                    "size": len(file_data)
                })
            )
            
            logger.info(f"‚úÖ Qwen —Å–æ—Ö—Ä–∞–Ω–∏–ª –¥–æ–∫—É–º–µ–Ω—Ç {document_id} –≤ Redis")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Redis: {e}")
            raise
    
    async def get_document_from_redis(
        self,
        document_id: str
    ) -> Optional[Dict[str, any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ Redis
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: Qwen ‚Üí –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –≤ Redis ‚Üí –ø–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            –î–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None
        """
        try:
            from app.core.redis_client import get_redis
            import base64
            
            redis = await get_redis()
            
            document_key = f"document:{document_id}"
            data = await redis.get(document_key)
            
            if data:
                document_data = json.loads(data)
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ñ–∞–π–ª
                file_data = base64.b64decode(document_data["data"])
                return {
                    "data": file_data,
                    "metadata": document_data.get("metadata", {}),
                    "size": document_data.get("size", 0)
                }
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ Redis: {e}")
            return None
    
    async def process_search_query(
        self,
        query: str,
        rag_service,
        db
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: Qwen ‚Üí RAG ‚Üí Postgres ‚Üí Qwen ‚Üí Redis ‚Üí –æ—Ç–≤–µ—Ç
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            rag_service: –≠–∫–∑–µ–º–ø–ª—è—Ä RAG —Å–µ—Ä–≤–∏—Å–∞
            db: Database session
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        """
        try:
            # Qwen –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ RAG
            logger.info(f"Qwen –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {query}")
            
            # RAG –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ Postgres
            chunks = await rag_service.search_for_qwen(db, query, top_k=5)
            
            if not chunks:
                return {
                    "answer": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.",
                    "documents": [],
                    "chunks": []
                }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞
            context = "\n\n".join([
                f"–î–æ–∫—É–º–µ–Ω—Ç: {chunk['document_title']}\n{chunk['text'][:200]}"
                for chunk in chunks[:3]
            ])
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {query}

–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."""
            
            answer = self._generate_text(
                prompt=prompt,
                max_new_tokens=256,
                temperature=0.7
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Redis
            documents = []
            for chunk in chunks[:3]:  # –ë–µ—Ä–µ–º —Ç–æ–ø-3 –¥–æ–∫—É–º–µ–Ω—Ç–∞
                doc_id = chunk["document_id"]
                doc_data = await self.get_document_from_redis(doc_id)
                if doc_data:
                    documents.append({
                        "document_id": doc_id,
                        "title": chunk["document_title"],
                        "type": chunk["document_type"],
                        "path": chunk.get("document_path"),
                        "available": True
                    })
            
            logger.info(f"‚úÖ Qwen —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å, –Ω–∞–π–¥–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            return {
                "answer": answer,
                "documents": documents,
                "chunks": chunks,
                "query": query
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return {
                "answer": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}",
                "documents": [],
                "chunks": []
            }
    
    def classify_document(self, text: str, filename: str = "") -> Dict[str, Any]:
        """Legacy method for direct classification"""
        try:
            self._ensure_model_loaded()
            return self._fallback_classify(text, filename)
        except Exception as e:
            logger.warning(f"Model not available, using fallback: {e}")
            return self._fallback_classify(text, filename)
    
    def _generate_text(
        self,
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """Generate text using Qwen model"""
        if self._model is None or self._tokenizer is None:
            raise RuntimeError("Model not loaded")
        
        try:
            inputs = self._tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            )
            
            device = self._get_best_device()
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            # For CPU, inputs stay on CPU (default)
            
            with torch.no_grad():
                outputs = self._model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self._tokenizer.pad_token_id,
                    eos_token_id=self._tokenizer.eos_token_id
                )
            
            generated_text = self._tokenizer.decode(
                outputs[0],
                skip_special_tokens=True
            )
            
            if generated_text.startswith(prompt):
                generated_text = generated_text[len(prompt):].strip()
            
            return generated_text
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            raise
    
    def _fallback_classify(self, text: str, filename: str) -> Dict[str, Any]:
        """Fallback classification based on keywords"""
        text_lower = text.lower()
        filename_lower = filename.lower()
        
        doc_type = "scan"
        if any(word in text_lower for word in ["–¥–æ–≥–æ–≤–æ—Ä", "–∫–æ–Ω—Ç—Ä–∞–∫—Ç", "—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ"]):
            doc_type = "contract"
        elif any(word in text_lower for word in ["—Å—á–µ—Ç", "invoice", "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞"]):
            doc_type = "invoice"
        elif any(word in text_lower for word in ["–∞–∫—Ç", "–ø—Ä–∏–µ–º–∫–∏", "–≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"]):
            doc_type = "act"
        elif any(word in text_lower for word in ["–ø—Ä–∏–∫–∞–∑", "—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ", "order"]):
            doc_type = "order"
        elif any(word in text_lower for word in ["–ø–∏—Å—å–º–æ", "email", "—Å–æ–æ–±—â–µ–Ω–∏–µ"]):
            doc_type = "email"
        
        priority = "medium"
        if any(word in text_lower for word in ["—Å—Ä–æ—á–Ω–æ", "urgent", "–≤–∞–∂–Ω–æ", "important"]):
            priority = "high"
        elif any(word in text_lower for word in ["–Ω–∏–∑–∫–∏–π", "low", "–Ω–µ–≤–∞–∂–Ω–æ"]):
            priority = "low"
        
        return {
            "type": doc_type,
            "counterparty_name": None,
            "date": None,
            "priority": priority,
            "description": f"–î–æ–∫—É–º–µ–Ω—Ç: {filename or '–±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'}"
        }


# Singleton instance
qwen_service = QwenService()
