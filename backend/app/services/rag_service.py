import logging
import json
from typing import List, Dict, Optional
import numpy as np
import torch
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, text
from app.core.config import settings
from app.models.vector_store import DocumentChunk
from app.models.document import Document
from app.services.qwen_service import QwenService

logger = logging.getLogger(__name__)


class RAGService:
    """Service for RAG operations using Qwen3-4B for embeddings"""
    
    _instance = None
    _qwen_service = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RAGService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        # Lazy loading - Qwen –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        pass
    
    def _ensure_qwen_loaded(self):
        """Ensure Qwen model is loaded (lazy loading)"""
        if self._qwen_service is None:
            self._qwen_service = QwenService()
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            self._qwen_service._ensure_model_loaded()
    
    def generate_embedding(self, text: str) -> np.ndarray:
        """
        Generate embedding for text using Qwen3-4B model
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        self._ensure_qwen_loaded()
        
        try:
            model = self._qwen_service._model
            tokenizer = self._qwen_service._tokenizer
            
            if model is None or tokenizer is None:
                raise RuntimeError("Qwen model not loaded")
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            inputs = tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            )
            
            device = self._qwen_service._get_best_device()
            
            # –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å MPS
            # MPS –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, register_pytree_node)
            # –ü–æ—ç—Ç–æ–º—É –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            inputs_cpu = {k: v.to("cpu") for k, v in inputs.items()}
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ CPU
            with torch.no_grad():
                # –í—Ä–µ–º–µ–Ω–Ω–æ –ø–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                original_device = next(model.parameters()).device
                model_cpu = model.to("cpu")
                
                try:
                    outputs = model_cpu(**inputs_cpu, output_hidden_states=True)
                finally:
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                    model.to(original_device)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ (L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
            
            return embedding.astype(np.float32)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ Qwen: {e}")
            raise
    
    async def process_document_for_metrics(
        self,
        text: str,
        filename: str,
        file_size: int
    ) -> Dict[str, any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ Qwen
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: RAG –ø–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ –ø–µ—Ä–µ–¥–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ Qwen
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            filename: –ò–º—è —Ñ–∞–π–ª–∞
            file_size: –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –±–∞–π—Ç–∞—Ö
            
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ Qwen
        """
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        embedding = self.generate_embedding(text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        from app.services.document_processor import DocumentProcessor
        processor = DocumentProcessor()
        chunks = processor.chunk_text(text, {
            "file_name": filename,
            "file_size": file_size
        })
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = {
            "text": text,
            "filename": filename,
            "file_size": file_size,
            "text_length": len(text),
            "chunks_count": len(chunks),
            "embedding": embedding.tolist(),
            "chunks": [
                {
                    "text": chunk["text"],
                    "start_pos": chunk["start_pos"],
                    "end_pos": chunk["end_pos"]
                }
                for chunk in chunks
            ]
        }
        
        logger.info(f"‚úÖ RAG –æ–±—Ä–∞–±–æ—Ç–∞–ª –¥–æ–∫—É–º–µ–Ω—Ç {filename}, –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Qwen")
        return metrics
    
    def generate_embeddings_batch(self, texts: List[str], batch_size: int = None) -> List[np.ndarray]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è A100 GPU)
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –¥–ª—è A100)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        self._ensure_qwen_loaded()
        
        try:
            model = self._qwen_service._model
            tokenizer = self._qwen_service._tokenizer
            
            if model is None or tokenizer is None:
                raise RuntimeError("Qwen model not loaded")
            
            device = self._qwen_service._get_best_device()
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä batch_size –¥–ª—è A100
            if batch_size is None:
                if device == "cuda":
                    # A100 –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –±–∞—Ç—á–∏
                    batch_size = min(128, max(32, len(texts) // 5))
                elif device == "mps":
                    # MPS (Apple Silicon) - –º–µ–Ω—å—à–∏–µ –±–∞—Ç—á–∏
                    batch_size = min(16, max(4, len(texts) // 10))
                else:
                    # CPU - –µ—â–µ –º–µ–Ω—å—à–∏–µ –±–∞—Ç—á–∏
                    batch_size = min(8, max(2, len(texts) // 20))
            
            logger.info(f"üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞ {device} –±–∞—Ç—á–∞–º–∏ –ø–æ {batch_size}...")
            
            embeddings = []
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞
                inputs = tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=2048
                )
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU (CUDA) –¥–ª—è A100, –∏–Ω–∞—á–µ CPU –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                if device == "cuda":
                    # A100: –∏—Å–ø–æ–ª—å–∑—É–µ–º GPU –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    inputs_gpu = {k: v.to(device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        # –ú–æ–¥–µ–ª—å —É–∂–µ –Ω–∞ GPU
                        outputs = model(**inputs_gpu, output_hidden_states=True)
                        
                        hidden_states = outputs.hidden_states[-1]  # [batch_size, seq_len, hidden_size]
                        attention_mask = inputs_gpu.get('attention_mask', None)
                        
                        if attention_mask is not None:
                            mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()
                            sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)
                            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)
                            batch_embeddings = sum_hidden / sum_mask
                        else:
                            batch_embeddings = torch.mean(hidden_states, dim=1)
                        
                        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ CPU –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ numpy
                        batch_embeddings = batch_embeddings.cpu()
                else:
                    # CPU –∏–ª–∏ MPS: –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    inputs_cpu = {k: v.to("cpu") for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        original_device = next(model.parameters()).device
                        model_cpu = model.to("cpu")
                        
                        try:
                            outputs = model_cpu(**inputs_cpu, output_hidden_states=True)
                        finally:
                            model.to(original_device)
                        
                        hidden_states = outputs.hidden_states[-1]
                        attention_mask = inputs_cpu.get('attention_mask', None)
                        
                        if attention_mask is not None:
                            mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()
                            sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)
                            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)
                            batch_embeddings = sum_hidden / sum_mask
                        else:
                            batch_embeddings = torch.mean(hidden_states, dim=1)
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                for emb in batch_embeddings:
                    emb_np = emb.numpy().flatten()
                    norm = np.linalg.norm(emb_np)
                    if norm > 0:
                        emb_np = emb_np / norm
                    embeddings.append(emb_np.astype(np.float32))
            
            return embeddings
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–∞—Ç—á–µ–º: {e}")
            raise
    
    async def save_metrics_to_postgres(
        self,
        db: AsyncSession,
        document_id: str,
        metrics: Dict[str, any],
        classification_result: Dict[str, any]
    ):
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç Qwen –≤ Postgres
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: Qwen —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ‚Üí RAG ‚Üí Postgres
        
        Args:
            db: Database session
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metrics: –ò—Å—Ö–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            classification_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Ç Qwen
        """
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
            chunks = metrics.get("chunks", [])
            embedding = metrics.get("embedding")
            
            from app.models.vector_store import DocumentChunk
            import uuid
            
            if not chunks:
                logger.warning(f"–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}")
                return
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è A100)
            chunk_texts = [chunk_data["text"] for chunk_data in chunks]
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä batch_size (–¥–ª—è A100 –±—É–¥–µ—Ç 32-128)
            chunk_embeddings = self.generate_embeddings_batch(chunk_texts, batch_size=None)
            logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(chunk_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
            for i, (chunk_data, chunk_embedding) in enumerate(zip(chunks, chunk_embeddings)):
                chunk = DocumentChunk(
                    id=uuid.uuid4(),
                    document_id=uuid.UUID(document_id),
                    chunk_id=i,
                    text=chunk_data["text"],
                    start_pos=chunk_data["start_pos"],
                    end_pos=chunk_data["end_pos"],
                    embedding=chunk_embedding.tolist(),
                    chunk_metadata=json.dumps({
                        "filename": metrics.get("filename"),
                        "classification": classification_result
                    })
                )
                
                db.add(chunk)
            
            await db.commit()
            logger.info(f"‚úÖ RAG —Å–æ—Ö—Ä–∞–Ω–∏–ª {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ Postgres –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}")
            
        except Exception as e:
            await db.rollback()
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫ –≤ Postgres: {e}")
            raise
    
    async def search_for_qwen(
        self,
        db: AsyncSession,
        query: str,
        top_k: int = None
    ) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –¥–ª—è Qwen: RAG –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ Postgres
        –°–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ: Qwen ‚Üí RAG ‚Üí Postgres
        
        Args:
            db: Database session
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        if top_k is None:
            top_k = settings.RAG_TOP_K
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.generate_embedding(query)
            
            # –ü–æ–∏—Å–∫ –≤ Postgres —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –¥–ª—è pgvector —Å asyncpg
            embedding_list = query_embedding.tolist()
            embedding_str = '[' + ','.join(map(str, embedding_list)) + ']'
            
            query_sql = f"""
                SELECT 
                    dc.id,
                    dc.document_id,
                    dc.chunk_id,
                    dc.text,
                    dc.start_pos,
                    dc.end_pos,
                    dc.chunk_metadata,
                    d.title as document_title,
                    d.type as document_type,
                    d.path as document_path,
                    1 - (dc.embedding <=> '{embedding_str}'::vector) as similarity
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.is_deleted = false
                ORDER BY similarity DESC
                LIMIT {top_k}
            """
            
            result = await db.execute(text(query_sql))
            
            chunks = []
            for row in result:
                chunks.append({
                    "chunk_id": str(row.id),
                    "document_id": str(row.document_id),
                    "chunk_index": row.chunk_id,
                    "text": row.text,
                    "start_pos": row.start_pos,
                    "end_pos": row.end_pos,
                    "metadata": json.loads(row.chunk_metadata) if row.chunk_metadata else {},
                    "document_title": row.document_title,
                    "document_type": row.document_type,
                    "document_path": row.document_path,
                    "similarity": float(row.similarity)
                })
            
            logger.info(f"‚úÖ RAG –Ω–∞—à–µ–ª {len(chunks)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è Qwen")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–ª—è Qwen: {e}")
            raise
    
    async def add_document_chunks(
        self,
        db: AsyncSession,
        document_id: str,
        chunks: List[Dict]
    ):
        """Add document chunks with embeddings to vector store (legacy method)"""
        from app.models.vector_store import DocumentChunk
        import uuid
        
        try:
            for chunk_data in chunks:
                embedding = self.generate_embedding(chunk_data['text'])
                
                chunk = DocumentChunk(
                    id=uuid.uuid4(),
                    document_id=uuid.UUID(document_id),
                    chunk_id=chunk_data['chunk_id'],
                    text=chunk_data['text'],
                    start_pos=chunk_data['start_pos'],
                    end_pos=chunk_data['end_pos'],
                    embedding=embedding.tolist(),
                    chunk_metadata=json.dumps(chunk_data.get('metadata', {})) if chunk_data.get('metadata') else None
                )
                
                db.add(chunk)
            
            await db.commit()
            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}")
            
        except Exception as e:
            await db.rollback()
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —á–∞–Ω–∫–æ–≤: {e}")
            raise
    
    async def search_similar_chunks(
        self,
        db: AsyncSession,
        query: str,
        top_k: int = None,
        document_ids: Optional[List[str]] = None
    ) -> List[Dict]:
        """Search for similar chunks using vector similarity (legacy method)"""
        if top_k is None:
            top_k = settings.RAG_TOP_K
        
        try:
            query_embedding = self.generate_embedding(query)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –¥–ª—è pgvector —Å asyncpg
            embedding_list = query_embedding.tolist()
            embedding_str = '[' + ','.join(map(str, embedding_list)) + ']'
            
            query_sql = f"""
                SELECT 
                    dc.id,
                    dc.document_id,
                    dc.chunk_id,
                    dc.text,
                    dc.start_pos,
                    dc.end_pos,
                    dc.chunk_metadata,
                    d.title as document_title,
                    d.type as document_type,
                    1 - (dc.embedding <=> '{embedding_str}'::vector) as similarity
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.is_deleted = false
            """
            
            if document_ids:
                doc_ids_str = ','.join([f"'{str(doc_id)}'" for doc_id in document_ids])
                query_sql += f" AND dc.document_id = ANY(ARRAY[{doc_ids_str}]::uuid[])"
            
            query_sql += f" ORDER BY similarity DESC LIMIT {top_k}"
            
            result = await db.execute(text(query_sql))
            
            chunks = []
            for row in result:
                chunks.append({
                    "chunk_id": str(row.id),
                    "document_id": str(row.document_id),
                    "chunk_index": row.chunk_id,
                    "text": row.text,
                    "start_pos": row.start_pos,
                    "end_pos": row.end_pos,
                    "metadata": json.loads(row.chunk_metadata) if row.chunk_metadata else {},
                    "document_title": row.document_title,
                    "document_type": row.document_type,
                    "similarity": float(row.similarity)
                })
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            raise
    
    async def delete_document_chunks(
        self,
        db: AsyncSession,
        document_id: str
    ):
        """Delete all chunks for a document"""
        try:
            await db.execute(
                text("DELETE FROM document_chunks WHERE document_id = :document_id"),
                {"document_id": document_id}
            )
            await db.commit()
            logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω—ã —á–∞–Ω–∫–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}")
        except Exception as e:
            await db.rollback()
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —á–∞–Ω–∫–æ–≤: {e}")
            raise


# Singleton instance
rag_service = RAGService()
